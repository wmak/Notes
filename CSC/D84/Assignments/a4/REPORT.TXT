CSC D84 - Artificial Intelligence, Winter 2015

Assignment 4 - Reinforcement Learning - Q Learning

This assignment is worth:

15 AIUs (Artificial Intelligence Units)
toward the 35% assignment component of your final
mark.

________________________________________________

Student Name (last, first): Mak, William

Student number: 998992988

UTORid: makwill1

READ THIS AND SIGN YOUR NAME AT THE END:

 I certify that I have read the UTSC code on academic
honesty and plaguarism. All work submitted as part
of this assignment is my own.

	Signed: _William Mak__


(-5 marks for failing to provide the identifying
 information requested above)
________________________________________________

Answer the following questions. Be concise and clear
but explain carefully when needed.

1 .- (2 marks) Explain your reward function. What game elements
      are used, and why you think this is a good reward function
      for this problem.
	My reward function started simple, if the mouse lands on a cheese then
reward highly. If the mouse lands on a cat reward negatively. But after watching
this for a while I notec that my mouse would become cornered running away from
cats. So i tried to implement something so that my mouse would avoid corners.
And then i realized i should have my mouse avoid cats more strongly so i made
spots near the cat dangerous as well.

2 .- (3 marks) These are multiple experiments (once you are sure your 
     QLearning code is working!) 

     * IMPORTANT! * For each training run, SAVE the Qtable
     you will re-use the Qtables later.

     # Experiment 1, 10000 training rounds
     initGame(1522,1)
     doTrain(10000,20)
     # SAVE YOUR Q TABLE!
     doGame()
 
     Run at least 50 rounds of the game. 

     Record the mouse winning rate: 0.22

     # Experiment 2
     initGame(1522,1)
     doTrain(1000000,20)
     # SAVE YOUR Q TABLE!    <--- You have to submit this one as 'Qtable.pickle'
     doGame()			   

     Run at least 50 rounds of the game. 

     Record the mouse winning rate: 0.813559322034

     Would the mouse keep improving and become invincible if
     you ran, say, 100,000,000 training rounds per batch?

	 I don't think so, there still would exist scenarios where the mouse would
lose regardless of the actions that it might take
     

4 .- (3 marks) 

     Using the QTable for the training session with 1,000,000
     training trials:

     Record the mouse's winning rate for the following setups
     after 50 rounds of the game
     (NOTE: NO TRAINING THIS TIME AROUND. Re-use your Qtable)

     # 1
     initGame(4289,1)
     doGame()
	
     Mouse Winning Rate: 0.321428571429

     # 2
     initGame(31415,1)
     doGame()
	
     Mouse Winning Rate: 0.536585365854

     # 3
     initGame(3210,1)
     doGame()
	
     Mouse Winning Rate: 0.511682242991

     Average winning rate: 0.4565653934246667

     Explain the above numbers compared to the winning rates in 3),
     and provide some insight as to what is going on. 
	The QTable has been created based on the original seed. So at some
coordinate the best move is being blocked by the wall, and thus the mouse is
making suboptimal moves

5 .- (2 marks) Is standard Q-Learning a rasonable strategy for environments
     that change constantly? discuss based on the above

	No there is too much dependence on the makeup of the environment and not
enough on a case to case statement


7 .- (5 marks) Explain your feature set for feature-based Q Learning, and
               explain why and how each feature is expected to contribute
	       to helping your mouse win
	I kept my features simple. The distance to the closest cheese. The distance
to the closest cat but negatively. And the number of walls. The reason i chose
each of these is basically a feature version of what I learned from the first
implementation

8 .- (5 marks) Carry out the following experiments:

     Experiment 0: (Baseline)

     Train the mouse as follows:
     initGame(15,2,2,3)
     doQLearn(10,1)

     this is to establish a baseline performance without actually doing much training

     run the game once the training is done

     doGame()

     Record the mouse winning rate after 50 rounds: 0.037037037037

     Experiment 1:

     Train the mouse as follows:
     initGame(15,2,2,3)
     doQLearn(2500,10)     # <---- You are free to  use as many trials as you need so your mouse learns to win as much as possible
			   #       given your features

     * SAVE YOUR WEIGHTS FILE *  <---- You'll need to submit this file

     Then, start a new game:

     initGame(31415,2,2,3)
     doGame()

     Report mouse winning rate after 50 rounds: 0.241379310345

     Experiment 2: 

     initGame(31415,3,2,3)
     doGame()

     Report mouse winning rate after 50 rounds: 0.06

9 .- (5 marks) Based on the above, is feature-based learning better at
     dealing with changing environments than standard Q-learning?
     Provide a convincing (but short!) argument!
	No clearly not, on the larger map the mouse was very confused about what to
do.


_____________________________________________________

Mark with an 'x' where appropriate. If something is only
working partially, briefly describe what works, what
doesn't work, or what problems exist.
	
			Complete/Working	Partial		Not done

QLearn			x
 update

Reward			x
 function

Decide			x
 action

featureEval		x

evaluateQsa		x

maxQsa_prime	x

Qlearn_features	x

decideAction_features x

_____________________________________________________

Marking:

(5 marks) Implemented QLearn update.

(5 marks) Implemented a reasonable reward function

(5 marks)  Implemented the function that chooses the
           optimal action based on Qtable and
           maze

(15 marks) Implemented a non-trivial, good feature set
	   and evaluation function

(15 marks) Implemented a working feature-based Q-learning
	   algorithm

(20 marks) Competitive mouse performance

(25 marks) Answers in this report file

(- marks)  Penalty marks

Total for A4:       / out of 90


